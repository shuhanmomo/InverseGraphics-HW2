{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\OneDrive\\GSD 5th term\\Inverse Graphics\\HW2\\InverseGraphics-HW2\\src\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\OneDrive\\GSD 5th term\\Inverse Graphics\\HW2\\InverseGraphics-HW2\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from jaxtyping import Float\n",
    "from omegaconf import DictConfig\n",
    "from PIL import Image\n",
    "from torch import Tensor\n",
    "import torch\n",
    "from src.dataset.field_dataset import FieldDataset\n",
    "import numpy as np\n",
    "from tests.f32 import f32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FieldDatasetImage(FieldDataset):\n",
    "    def __init__(self, cfg: DictConfig) -> None:\n",
    "        \"\"\"Load the image in cfg.path into memory here.\"\"\"\n",
    "\n",
    "        super().__init__(cfg)\n",
    "        self.cfg = cfg\n",
    "        read_image = Image.open(cfg.path).convert(\"RGB\")\n",
    "\n",
    "        self.image = (\n",
    "            torch.tensor(np.array(read_image)).permute(2, 0, 1).unsqueeze(0).float()\n",
    "            / 255\n",
    "        )  # batch channel h w\n",
    "\n",
    "    def query(\n",
    "        self,\n",
    "        coordinates: Float[Tensor, \"batch d_coordinate\"],\n",
    "    ) -> Float[Tensor, \"batch d_out\"]:\n",
    "        \"\"\"Sample the image at the specified coordinates and return the corresponding\n",
    "        colors. Remember that the coordinates will be in the range [0, 1].\n",
    "\n",
    "        You may find the grid_sample function from torch.nn.functional helpful here.\n",
    "        Pay special attention to grid_sample's expected input range for the grid\n",
    "        parameter.\n",
    "        \"\"\"\n",
    "        coordinates = coordinates * 2 - 1\n",
    "        coordinates = coordinates.unsqueeze(0).unsqueeze(2)\n",
    "        sampled_colors = F.grid_sample(self.image, coordinates)  # batch channel d_out 1\n",
    "        return sampled_colors.squeeze(0).squeeze(-1).permute(1, 0)  # batch d_out\n",
    "\n",
    "    @property\n",
    "    def d_coordinate(self) -> int:\n",
    "        return 2\n",
    "\n",
    "    @property\n",
    "    def d_out(self) -> int:\n",
    "        return 3\n",
    "\n",
    "    @property\n",
    "    def grid_size(self) -> tuple[int, ...]:\n",
    "        \"\"\"Return a grid size that corresponds to the image's shape.\"\"\"\n",
    "        return self.image.shape[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sampling():\n",
    "    dataset = FieldDatasetImage(\n",
    "        DictConfig(\n",
    "            {\n",
    "                \"path\": \"data/tester.png\",\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "    coordinates = [\n",
    "        [7 / 16, 7 / 16],\n",
    "        [7 / 16, 9 / 16],\n",
    "        [9 / 16, 7 / 16],\n",
    "        [9 / 16, 9 / 16],\n",
    "    ]\n",
    "\n",
    "    expected = [\n",
    "        [1, 0, 0],\n",
    "        [0, 0, 1],\n",
    "        [1, 1, 0],\n",
    "        [0, 1, 0],\n",
    "    ]\n",
    "\n",
    "    assert torch.allclose(\n",
    "        dataset.query(f32(coordinates)),\n",
    "        f32(expected),\n",
    "    )\n",
    "    print(\"test successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\moumo\\anaconda3\\envs\\ighw2\\lib\\site-packages\\torch\\nn\\functional.py:4236: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "test_sampling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(\n",
    "        origins: Float[Tensor, \"batch 3\"],\n",
    "        directions: Float[Tensor, \"batch 3\"],\n",
    "        near: float,\n",
    "        far: float,\n",
    "        num_samples: int,\n",
    "    ) -> tuple[\n",
    "        Float[Tensor, \"batch sample 3\"],  # xyz sample locations\n",
    "        Float[Tensor, \"batch sample+1\"],  # sample boundaries\n",
    "    ]:\n",
    "        \"\"\"For each ray, equally divide the space between the specified near and far\n",
    "        planes into num_samples segments. Return the segment boundaries (including the\n",
    "        endpoints at the near and far planes). Also return sample locations, which fall\n",
    "        at the midpoints of the segments.\n",
    "        \"\"\"\n",
    "        num_rays = origins.shape[0]\n",
    "        t_vals = torch.linspace(0.0, 1.0, steps=num_samples+1)\n",
    "        z_vals = (far - near) * t_vals + near\n",
    "        z_vals = z_vals.expand([num_rays, num_samples+1])\n",
    "        mid_pts = 0.5 * (z_vals[..., 1:] + z_vals[..., :-1])\n",
    "        print(f\"mid_pts shape is {mid_pts.shape}\")\n",
    "        # stratified sampling\n",
    "        \n",
    "        np.random.seed(0)\n",
    "        t_rand = torch.rand(*mid_pts.shape)\n",
    "        upper = z_vals[..., 1:] - mid_pts\n",
    "        lower = mid_pts - z_vals[..., :-1]\n",
    "        z_vals_rand = mid_pts + upper * t_rand - lower * (1-t)*t_rand\n",
    "        pts = (\n",
    "            origins[..., None, :] + directions[..., None, :] * z_vals_rand[..., :, None]\n",
    "        )  # [N_rays, N_samples, 3]\n",
    "        boundaries = z_vals_rand\n",
    "\n",
    "        return pts, boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mid_pts shape is torch.Size([2, 4])\n",
      "Sample Points (pts):\n",
      "torch.Size([2, 5, 3])\n",
      "\n",
      "Boundaries:\n",
      "torch.Size([2, 5])\n"
     ]
    }
   ],
   "source": [
    "# Mock data for testing:\n",
    "origins = torch.tensor([[0.0, 0.0, 0.0], [1.0, 1.0, 1.0]])\n",
    "directions = torch.tensor([[0.0, 0.0, 1.0], [0.0, 0.0, -1.0]])\n",
    "near = 1.0\n",
    "far = 10.0\n",
    "num_samples = 4\n",
    "\n",
    "# Call the function\n",
    "pts, boundaries = generate_samples(origins, directions, near, far, num_samples)\n",
    "\n",
    "# Print results\n",
    "print(\"Sample Points (pts):\")\n",
    "print(pts.shape)\n",
    "print(\"\\nBoundaries:\")\n",
    "print(boundaries.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ighw2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
